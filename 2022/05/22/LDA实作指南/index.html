<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="LDA的动机和思路 让我们从下面这张图讲起：  生成过程是按照一定的概率选择topic，再从对应topic中按照一定概率选择token；而推断过程则是给定了一些文档和其中的token，不知道每个token是由哪个topic生成，需要去推断每篇文档取自各个主题的概率，以及每个主题在抽出各个词的概率。 这个推断不能胡乱推断，而应该根据一定的假设进行，这个假设就是LDA的生成过程（这也是为什么许多介绍L">
<meta property="og:type" content="article">
<meta property="og:title" content="LDA实作指南">
<meta property="og:url" content="https://zll17.github.io/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/index.html">
<meta property="og:site_name" content="Leilan&#39;s Blog">
<meta property="og:description" content="LDA的动机和思路 让我们从下面这张图讲起：  生成过程是按照一定的概率选择topic，再从对应topic中按照一定概率选择token；而推断过程则是给定了一些文档和其中的token，不知道每个token是由哪个topic生成，需要去推断每篇文档取自各个主题的概率，以及每个主题在抽出各个词的概率。 这个推断不能胡乱推断，而应该根据一定的假设进行，这个假设就是LDA的生成过程（这也是为什么许多介绍L">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zll17.github.io/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/lda-01.png">
<meta property="og:image" content="https://zll17.github.io/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/lda-02.png">
<meta property="article:published_time" content="2022-05-22T08:22:58.000Z">
<meta property="article:modified_time" content="2022-06-29T12:34:48.972Z">
<meta property="article:author" content="zhangleilan">
<meta property="article:tag" content="TopicModel EM算法 MCMC DIY">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zll17.github.io/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/lda-01.png">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>LDA实作指南</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
			tex2jax: {
			  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			  inlineMath: [['$','$']]
			}
		  });
		</script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
	
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"></head>

<body class="max-width mx-auto px3 ltr">    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" "Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/projects/">Projects</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        
        <li><a class="icon" aria-label="Next post " href="/2022/05/22/Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%8F%8A%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top " href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post " href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://zll17.github.io/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://zll17.github.io/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/&text=LDA实作指南"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://zll17.github.io/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/&title=LDA实作指南"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://zll17.github.io/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/&is_video=false&description=LDA实作指南"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=LDA实作指南&body=Check out this article: https://zll17.github.io/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://zll17.github.io/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/&title=LDA实作指南"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://zll17.github.io/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/&title=LDA实作指南"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://zll17.github.io/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/&title=LDA实作指南"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://zll17.github.io/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/&title=LDA实作指南"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://zll17.github.io/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/&name=LDA实作指南&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://zll17.github.io/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/&t=LDA实作指南"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#LDA%E7%9A%84%E5%8A%A8%E6%9C%BA%E5%92%8C%E6%80%9D%E8%B7%AF"><span class="toc-number">1.</span> <span class="toc-text">LDA的动机和思路</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">2.</span> <span class="toc-text">参考文献</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        LDA实作指南
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">zhangleilan</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2022-05-22T08:22:58.000Z" itemprop="datePublished">2022-05-22</time>
        
      
    </div>


      

      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/TopicModel-EM%E7%AE%97%E6%B3%95-MCMC-DIY/" rel="tag">TopicModel EM算法 MCMC DIY</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h4 id="LDA的动机和思路">LDA的动机和思路</h4>
<p>让我们从下面这张图讲起：</p>
<img src="/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/lda-01.png" class="" title="Topic Model的生成和推断过程">
<p>生成过程是按照一定的概率选择topic，再从对应topic中按照一定概率选择token；而推断过程则是给定了一些文档和其中的token，不知道每个token是由哪个topic生成，需要去推断每篇文档取自各个主题的概率，以及每个主题在抽出各个词的概率。</p>
<p>这个推断不能胡乱推断，而应该根据一定的假设进行，这个假设就是LDA的生成过程（这也是为什么许多介绍LDA的文章会在一开始就把LDA的生成过程写出来）。</p>
<img src="/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/lda-02.png" class="" title="LDA生成过程的板块表示">
<p>根据上述生成过程，我们可以看出，其中最重要的就是给出参数$\Theta$和$\Phi$的估计，也就是$\theta_{m}$和$\phi_{k}$的估计。这两个参数的估计都可以通过主题分配向量$\textbf{z}$的采样来得到，也就是说，如果能够“合理”地给出每个词的主题分配情况（即$\textbf{z}$的若干采样），那么就能推断出$\theta$和$\phi$。</p>
<p>于是问题就变成，在给定观测数据$\textbf{w}$和先验超参数$\alpha$和$\beta$的前提下，如何推断$\textbf{z}$的分布（进而可以从中采样），也就是求：</p>
<p>$p(\textbf{z}|\textbf{w},\alpha,\beta)$，</p>
<p>由Bayes法则， $p(\textbf{z}|\textbf{w},\alpha,\beta)=\frac{p(\textbf{z},\textbf{w}|\alpha,\beta)}{p(\textbf{w}|\alpha,\beta)}$，分母由于维数过高极其稀疏而难以估计，注意到在对$\textbf{z}$的采样中，只需求得$\textbf{z}$在各主题上的相对概率大小即可，即$p(\textbf{z}|\textbf{w},\alpha,\beta)=\frac{p(\textbf{z},\textbf{w}|\alpha,\beta)}{p(\textbf{w}|\alpha,\beta)} \propto p(\textbf{z},\textbf{w}|\alpha,\beta)$，</p>
<p>因此现在关键在算$p(\textbf{z},\textbf{w}|\alpha,\beta)$，</p>
<p>根据生成过程，给定$\alpha$和$\beta$，要得到$\textbf{z}$和$\textbf{w}$，中间有隐变量$\theta$和$\phi$，因此上述条件概率需要将这两个隐变量通过积分消去（即求边缘概率）,</p>
<p>$p(\textbf{z},\textbf{w}|\alpha,\beta)=\int{\int{p(\textbf{z},\textbf{w},\theta,\phi|\alpha,\beta) \mathrm{d}\theta} \mathrm{d}\phi} = \int{\int{p(\phi|\beta)p(\theta|\alpha)p(\textbf{z}|\theta)p(\textbf{w}|\phi_{z})\mathrm{d}\theta}\mathrm{d}\phi} = \int{p(\textbf{z}|\theta)p(\theta|\alpha)\mathrm{d}\theta}\int{p(\textbf{w}|\phi_{z})p(\phi|\beta)\mathrm{d}\phi}$，（也即=$p(\textbf{z}|\alpha)p(\textbf{w}|\textbf{z},\beta)$）</p>
<p>分别推导这两项：</p>
<p>记$\theta_{mk}$是第$m$个文本生成第$k$个主题的概率，$n_{mk}$是语料库中第$m$的文本生成第$k$个主题的次数，</p>
<p>第一项：</p>
<p>$$<br>
\begin{align*}<br>
p(\mathbf{z} \mid \alpha) &amp;=<br>
\int p(\mathbf{z} \mid \theta) p(\theta \mid \alpha) \mathrm{d} \theta \\<br>
&amp;=\int \prod_{m=1}^{M} \frac{1}{\mathrm{~B}(\alpha)} \prod_{k=1}^{K} \theta_{m k}^{n_{m k}+\alpha_{k}-1} \mathrm{~d} \theta \\<br>
&amp;=\prod_{m=1}^{M} \frac{1}{\mathrm{~B}(\alpha)} \int \prod_{k=1}^{K} \theta_{m k}^{n_{m k}+\alpha_{k}-1} \mathrm{~d} \theta \\<br>
&amp;=\prod_{m=1}^{M} \frac{\mathrm{B}\left(n_{m}+\alpha\right)}{\mathrm{B}(\alpha)} \\<br>
\end{align*}<br>
$$<br>
，</p>
<p>(此处$p(\mathbf{z} \mid \theta) = \prod_{m=1}^{M} \prod_{k=1}^{K} \theta_{m k}^{n_{m k}}$，而由假设：$\theta$服从参数为$\alpha$的Dirichlet分布，因此$\theta$的先验分布$p(\theta \mid \alpha)=\frac{\Gamma\left(\sum_{i=1}^{k} \alpha_{i}\right)}{\prod_{i=1}^{k} \Gamma\left(\alpha_{i}\right)} \prod_{i=1}^{k} \theta_{i}^{\alpha_{i}-1}=\frac{1}{\mathrm{~B}(\alpha)} \prod_{i=1}^{k} \theta_{i}^{\alpha_{i}-1}=\operatorname{Dir}(\theta \mid \alpha)$，推导参考[1])。</p>
<p>第二项：</p>
<p>$$<br>
\begin{align*}<br>
p(\mathbf{w} \mid \mathbf{z}, \beta) &amp;=\int p(\mathbf{w} \mid \mathbf{z}, \varphi) p(\varphi \mid \beta) \mathrm{d} \varphi \\<br>
&amp;=\int \prod_{k=1}^{K} \frac{1}{\mathrm{~B}(\beta)} \prod_{v=1}^{V} \varphi_{k v}^{n_{k v}+\beta_{v}-1} \mathrm{~d} \varphi \\<br>
&amp;=\prod_{k=1}^{K} \frac{1}{\mathrm{~B}(\beta)} \int \prod_{v=1}^{V} \varphi_{k v}^{n_{k v}+\beta_{v}-1} \mathrm{~d} \varphi \\<br>
&amp;=\prod_{k=1}^{K} \frac{\mathrm{B}\left(n_{k}+\beta\right)}{\mathrm{B}(\beta)}<br>
\end{align*}<br>
$$<br>
，</p>
<p>其中$n_{k}=(n_{k1},n_{k2},\dots,n_{kV})$，（此处$p(\mathbf{w} \mid \mathbf{w}, \phi) = \prod_{k=1}^{K} \prod_{v=1}^{V} \phi_{kv}^{n_{kv}}$，同样由假设：$\phi$服从参数为$\beta$的Dirichlet分布，因此$\phi$的先验分布$p(\phi_{k} \mid \beta)=\frac{\Gamma\left(\sum_{i=1}^{V} \beta_{i}\right)}{\prod_{i=1}^{V} \Gamma\left(\beta_{i}\right)} \prod_{v=1}^{V} \phi_{kv}^{\beta_{v}-1}=\frac{1}{\mathrm{~B}(\beta)} \prod_{v=1}^{V} \phi_{kv}^{\beta_{v}-1}=\operatorname{Dir}(\phi \mid \beta))$）</p>
<p>可知：$p(\textbf{z},\textbf{w}|\alpha,\beta)=\prod_{d}\frac{B(n_{d,.}+\alpha)}{B(\alpha)} \prod_{k}\frac{B(n_{k,.}+\beta)}{B(\beta)}$，即$p(\textbf{z} |\textbf{w},\alpha,\beta) \propto \prod_{d}\frac{B(n_{d,.}+\alpha)}{B(\alpha)} \prod_{k}\frac{B(n_{k,.}+\beta)}{B(\beta)}$</p>
<p>于是我们可以据上式采样$p(\textbf{z}|\textbf{w},\alpha,\beta)$。直接采样是可以的，但高维空间效率极低，因此可以Gimbbs采样，一维一维地采，因此需要估计$p(z_{i}|\textbf{z}_{-i},\textbf{w},\alpha,\beta)$，易知：</p>
<p>$$<br>
\begin{equation}<br>
p(z_{i}|\textbf{z}<em>{-i},\textbf{w},\alpha,\beta)<br>
=\frac{p(z</em>{i},\textbf{z}<em>{-i}|\textbf{w},\alpha,\beta)}{p(\textbf{z}</em>{-i}|\textbf{w},\alpha,\beta)}<br>
=\frac{p(\textbf{z}|\textbf{w},\alpha,\beta)}{p(\textbf{z}_{-i}|\textbf{w},\alpha,\beta)}<br>
\end{equation}<br>
$$<br>
，</p>
<p>分母 $p(\textbf{z}<em>{-i},\textbf{w},\alpha,\beta)$ 记作$Z</em>{i}$，$Z_{i}$为将位置$i$处的主题排除后的边缘概率，注意到$Z_{i}$与$z_{i}$的取值无关（因为$z_{i}$被排除了），即 $Z_{zi=t1}=Z_{zi=t2}(t1 \neq t2)$ ，因此</p>
<p>$p(z_{i}|\textbf{z}<em>{-i},\textbf{w},\alpha,\beta)=\frac{p(\textbf{z}|\textbf{w},\alpha,\beta)}{Z</em>{i}} \propto p(\textbf{z} | \textbf{w},\alpha,\beta) \propto \prod_{d}\frac{B(n_{d,.}+\alpha)}{B(\alpha)} \prod_{k}\frac{B(n_{k,.}+\beta)}{B(\beta)}$</p>
<p>即 $p(z_{i}|\textbf{z}<em>{-i},\textbf{w},\alpha,\beta) \propto \frac{n</em>{kv}+\beta_{v}}{\sum_{v=1}^{V}(n_{kv}+\beta_{v})} \frac{n_{mk}+\alpha_{k}}{\sum_{k=1}^{K}(n_{mk}+\alpha_{k})}$，（此步骤推断可参考文献[^2]P35）</p>
<p>到这里，LDA的Gibbs采样算法就呼之欲出了：</p>
<p>（以下摘自李航《统计学习方法》[^1]）</p>
<ul>
<li>
<p>Input：语料库的单词序列$\textbf{w}={\textbf{w}<em>{1},\textbf{w}</em>{2},\dots,\textbf{w}_{M}}$,</p>
</li>
<li>
<p>Output: 语料库的主题序列$\textbf{z}={\textbf{z}<em>{1},\textbf{z}</em>{2},\dots,\textbf{z}_{M}}$，估计模型参数$\theta$和$\phi$。</p>
</li>
<li>
<p>超参数：主题数$K$，Dirichlet参数$\alpha$和$\beta$。</p>
</li>
<li>
<p>初始化：</p>
<ul>
<li>“文档-主题”计数矩阵记为$C_{MK}$，其元素$C_{mk}$表示文档$m$中主题$k$的出现次数</li>
<li>“主题-词”计数矩阵记为$P_{KV}$，其元素$P_{kv}$表示主题$k$中单词$v$的出现次数</li>
<li>“文档-主题和”计数向量记为$(C_{1},C_{2},\dots,C_{M})$，其元素$C_{m}$表示文档$m$中包含的主题数</li>
<li>“主题-词”计数向量记为$(P_{1},P_{2},\dots,P_{K})$，其元素$P_{k}$表示主题$k$中包含的单词数</li>
</ul>
<p>以上变量均初始化为0，接着执行以下步骤：</p>
<p>对所有文档$\textbf{w}_{m}$:</p>
<p>​		对该文档中的所有单词$w_{mn},(n=1,2,\dots,N_{m})$：</p>
<p>​				基于多项分布抽样主题$z_{mn}=z_{k}\sim Mult(\frac{1}{K})$</p>
<p>​				$C_{mk} += 1$, $C_{m}+=1$, $P_{kv}+=1$, $P_{k}+=1$</p>
</li>
<li>
<p>学习：</p>
<p>对所有文档$\textbf{w}_{m}$：</p>
<p>​		对该文档中的所有单词$w_{mn}$：</p>
<p>​			设当前单词在词表中索引为$v$，所分配的主题$z_{mn}$的主题索引为$k$</p>
<p>​			$C_{mk} -= 1$, $C_{m}-=1$, $P_{kv}-=1$, $P_{k}-=1$</p>
<p>​			根据条件分布进行抽样：</p>
<p>$p(z_{i}|\textbf{z}<em>{-i},\textbf{w},\alpha,\beta) \propto \frac{n</em>{kv}+\beta_{v}}{\sum_{v=1}^{V}(n_{kv}+\beta_{v})} \frac{n_{mk}+\alpha_{k}}{\sum_{k=1}^{K}(n_{mk}+\alpha_{k})}$，</p>
<p>​			设抽样得到主题$k^{\prime}$，令$z_{mn}=k^{\prime}$，</p>
<p>​			$C_{mk^{\prime}} += 1$, $C_{m}+=1$, $P_{k^{\prime}v}+=1$, $P_{k^{\prime}}+=1$</p>
<p>重复上述步骤，直至度过燃烧期，当收敛后，得到$\textbf{z}$的若干样本，于是可以据此估计参数$\theta$和$\phi$，注意是用分布的期望进行的估计，</p>
</li>
</ul>
<p>最终得到$\theta_{mk}=\frac{n_{mk}+\alpha_{k}}{\sum_{k=1}^{K}(n_{mk}+\alpha_{k})}$，以及 $\phi_{kv}=\frac{n_{kv}+\beta_{v}}{\sum_{v=1}^{V}(n_{kv}+\beta_{v})}$。</p>
<p>​		算法完毕。</p>
<p>当有新文档来时，保留$\phi$，如上更新$\theta$，得到文档的主题分布。</p>
<p>估计参数$\theta$时，由LDA的定义，有</p>
<p>$p\left(\theta_{m} \mid \mathbf{z}<em>{m}, \alpha\right)=\frac{1}{Z</em>{\theta_{m}}} \prod_{n=1}^{N_{m}} p\left(z_{m n} \mid \theta_{m}\right) p\left(\theta_{m} \mid \alpha\right)=\operatorname{Dir}\left(\theta_{m} \mid n_{m}+\alpha\right)$，由Dirichlet分布的性质（参考[2]）,得到</p>
<p>$\theta_{mk}=\frac{n_{mk}+\alpha_{k}}{\sum_{k=1}^{K}(n_{mk}+\alpha_{k})}$,</p>
<p>估计参数$\phi$时，有</p>
<p>$p\left(\varphi_{k} \mid \mathbf{w}, \mathbf{z}, \beta\right)=\frac{1}{Z_{\varphi_{k}}} \prod_{i=1}^{I} p\left(w_{i} \mid \varphi_{k}\right) p\left(\varphi_{k} \mid \beta\right)=\operatorname{Dir}\left(\varphi_{k} \mid n_{k}+\beta\right)$，同样由Dirichlet分布的性质，得到</p>
<p>$\phi_{kv}=\frac{n_{kv}+\beta_{v}}{\sum_{v=1}^{V}(n_{kv}+\beta_{v})}$</p>
<p>根据上述算法，可以很容易地写出程序，需要说明的是，本文的程序实现尽可能从易于理解的角度出发，并不打算作太多优化。这里以数据集作为示例数据。</p>
<p>首先进行分词，分词器选择HanLP；并去掉特殊符号和停用词，停用词表选用hit_stopwords。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> pyhanlp <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">stopwords = <span class="built_in">set</span>([line.strip(<span class="string">&#x27;\n&#x27;</span>) <span class="keyword">for</span> line <span class="keyword">in</span> <span class="built_in">open</span>(<span class="string">&#x27;cn_stopwords.txt&#x27;</span>,<span class="string">&#x27;r&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>).readlines()])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">sent</span>):</span><br><span class="line">    pat = re.<span class="built_in">compile</span>(<span class="string">r&#x27;[0-9!&quot;#$%&amp;\&#x27;()*+,-./:;&lt;=&gt;?@—，。：★、￥…【】（）《》？“”‘’！\[\\\]^_`&#123;|&#125;~\u3000]+&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> [t.word <span class="keyword">for</span> t <span class="keyword">in</span> HanLP.segment(sent) <span class="keyword">if</span> pat.search(t.word)==<span class="literal">None</span> <span class="keyword">and</span> t.word.strip()!=<span class="string">&#x27;&#x27;</span> <span class="keyword">and</span> <span class="keyword">not</span> (t.word <span class="keyword">in</span> stopwords)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_docs</span>(<span class="params">filename,n_samples=-<span class="number">1</span></span>):</span><br><span class="line">    tokenized_docs = []</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filename,<span class="string">&#x27;r&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> rfp:</span><br><span class="line">        lines = [line.strip(<span class="string">&#x27;\n&#x27;</span>) <span class="keyword">for</span> line <span class="keyword">in</span> rfp.readlines()]</span><br><span class="line">        lines = lines <span class="keyword">if</span> n_samples==-<span class="number">1</span> <span class="keyword">else</span> lines[:n_samples]</span><br><span class="line">        <span class="keyword">for</span> i,line <span class="keyword">in</span> <span class="built_in">enumerate</span>(lines):</span><br><span class="line">            sent = json.loads(line)[<span class="string">&#x27;sentence&#x27;</span>]</span><br><span class="line">            tokenized_docs.append(tokenize(sent))</span><br><span class="line">            <span class="keyword">if</span> i&lt;<span class="number">100</span> <span class="keyword">and</span> i%<span class="number">10</span>==<span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(tokenize(sent))</span><br><span class="line">    <span class="keyword">return</span> tokenized_docs</span><br><span class="line"></span><br><span class="line">trn_tokenized_docs = load_docs(<span class="string">&#x27;tnews_public/train.json&#x27;</span>,n_samples=<span class="number">1000</span>)</span><br><span class="line">new_tokenized_docs = load_docs(<span class="string">&#x27;tnews_public/dev.json&#x27;</span>,n_samples=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">❯ python .\LDA.py</span></span><br><span class="line"><span class="string">[&#x27;上课时&#x27;, &#x27;学生&#x27;, &#x27;手机&#x27;, &#x27;响&#x27;, &#x27;个&#x27;, &#x27;不停&#x27;, &#x27;老师&#x27;, &#x27;一怒之下&#x27;, &#x27;把&#x27;, &#x27;手机&#x27;, &#x27;摔&#x27;, &#x27;了&#x27;, &#x27;家长&#x27;, &#x27;拿&#x27;, &#x27;发票&#x27;, &#x27;让&#x27;, &#x27;老师&#x27;, &#x27;赔&#x27;, &#x27;大家&#x27;, &#x27;怎么&#x27;, &#x27;</span></span><br><span class="line"><span class="string">看待&#x27;, &#x27;这种&#x27;, &#x27;事&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;凌云&#x27;, &#x27;研发&#x27;, &#x27;的&#x27;, &#x27;国产&#x27;, &#x27;两&#x27;, &#x27;轮&#x27;, &#x27;电动车&#x27;, &#x27;怎么样&#x27;, &#x27;有&#x27;, &#x27;什么&#x27;, &#x27;惊喜&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;取名&#x27;, &#x27;困难&#x27;, &#x27;症&#x27;, &#x27;患者&#x27;, &#x27;皇马&#x27;, &#x27;的&#x27;, &#x27;贝尔&#x27;, &#x27;第&#x27;, &#x27;一个&#x27;, &#x27;受害者&#x27;, &#x27;就是&#x27;, &#x27;他&#x27;, &#x27;的&#x27;, &#x27;儿子&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;葫芦&#x27;, &#x27;都&#x27;, &#x27;能&#x27;, &#x27;做成&#x27;, &#x27;什么&#x27;, &#x27;乐器&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;中级会计&#x27;, &#x27;考试&#x27;, &#x27;每日&#x27;, &#x27;一练&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;复仇者&#x27;, &#x27;联盟&#x27;, &#x27;中&#x27;, &#x27;奇异&#x27;, &#x27;博士&#x27;, &#x27;为什么&#x27;, &#x27;不&#x27;, &#x27;用&#x27;, &#x27;时间&#x27;, &#x27;宝石&#x27;, &#x27;跟&#x27;, &#x27;灭&#x27;, &#x27;霸&#x27;, &#x27;谈判&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;拥抱&#x27;, &#x27;编辑&#x27;, &#x27;时代&#x27;, &#x27;内容&#x27;, &#x27;升级&#x27;, &#x27;为&#x27;, &#x27;产品&#x27;, &#x27;海内外&#x27;, &#x27;媒体&#x27;, &#x27;如何&#x27;, &#x27;规划&#x27;, &#x27;下&#x27;, &#x27;个&#x27;, &#x27;十&#x27;, &#x27;年&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;地球&#x27;, &#x27;这&#x27;, &#x27;是&#x27;, &#x27;怎么&#x27;, &#x27;了&#x27;, &#x27;美国&#x27;, &#x27;夏威夷&#x27;, &#x27;群岛&#x27;, &#x27;突发&#x27;, &#x27;级&#x27;, &#x27;地震&#x27;, &#x27;游客&#x27;, &#x27;紧急&#x27;, &#x27;疏散&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;定安&#x27;, &#x27;计划&#x27;, &#x27;用&#x27;, &#x27;三&#x27;, &#x27;年&#x27;, &#x27;时间&#x27;, &#x27;修复&#x27;, &#x27;全县&#x27;, &#x27;处&#x27;, &#x27;不&#x27;, &#x27;可移动&#x27;, &#x27;文物&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;军工&#x27;, &#x27;已&#x27;, &#x27;动真格&#x27;, &#x27;中航&#x27;, &#x27;科工&#x27;, &#x27;占据&#x27;, &#x27;着&#x27;, &#x27;舞台&#x27;, &#x27;正&#x27;, &#x27;中央&#x27;]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>接下来实现LDA模型，我们希望实现如下接口：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">lda_model = LDA(docs=tokenized_docs,K=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">lda_model.train()</span><br><span class="line"></span><br><span class="line">lda_model.add_docs(new_tokenized_docs)</span><br><span class="line"></span><br><span class="line">tp_wd_dist = lda_model.topic_word_dist() </span><br><span class="line"><span class="comment"># return [K,V] matrix</span></span><br><span class="line"></span><br><span class="line">lda_model.show_topic_words()</span><br><span class="line"></span><br><span class="line">doc_tp_dist = lda_model.get_corpus_dist()  </span><br><span class="line"><span class="comment"># return [M,K] matrix, where M = # of all the docs have feed in.</span></span><br><span class="line"></span><br><span class="line">doc_tp_dist = lda_model.batch_inference(new_tokenized_docs) </span><br><span class="line"><span class="comment"># return [M&#x27;,K] matrix, where M&#x27; = # of new_tokenized_docs</span></span><br><span class="line"></span><br><span class="line">tp_dist = lda_model.inference(new_tokenized_doc) </span><br><span class="line"><span class="comment"># return [1,K] matrix</span></span><br></pre></td></tr></table></figure>
<p>首先是初始化的部分，这部分可以很容易地完成，各关键部分的功能在注释中有所标注：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LDA</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,docs,K</span>):</span><br><span class="line">        <span class="comment"># params: docs: tokenized sentence list, e.g. [[&#x27;hello&#x27;,&#x27;world&#x27;],[&#x27;nice&#x27;,&#x27;job&#x27;],...]</span></span><br><span class="line">        <span class="comment"># params: K: number of topics</span></span><br><span class="line">        self.idx2token = []</span><br><span class="line">        self.token2idx = &#123;&#125;</span><br><span class="line">        self.M = <span class="built_in">len</span>(docs) </span><br><span class="line">        self.K = K</span><br><span class="line">        <span class="comment"># Build vocabulary</span></span><br><span class="line">        <span class="keyword">for</span> doc <span class="keyword">in</span> docs:</span><br><span class="line">            <span class="keyword">for</span> wd <span class="keyword">in</span> doc:</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> (wd <span class="keyword">in</span> self.idx2token):</span><br><span class="line">                    self.idx2token.append(wd)</span><br><span class="line">        <span class="keyword">for</span> idx,wd <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.idx2token):</span><br><span class="line">            self.token2idx[wd] = idx</span><br><span class="line">        self.V = <span class="built_in">len</span>(self.idx2token)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Vocabulary length: <span class="subst">&#123;self.V&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize count matrix and vectors</span></span><br><span class="line">        self.beta = np.ones(self.V) / self.V</span><br><span class="line">        self.alpha = np.ones(self.K) / self.K</span><br><span class="line">        self.matC = np.zeros((self.M,self.K))</span><br><span class="line">        self.matP = np.zeros((self.K,self.V))</span><br><span class="line">        self.vecC = np.zeros(self.M)</span><br><span class="line">        self.vecP = np.zeros(self.K)</span><br><span class="line">        self.zs = []</span><br><span class="line">        self.k_ids = <span class="built_in">list</span>(<span class="built_in">range</span>(self.K)) <span class="comment"># ids of topics</span></span><br><span class="line">        self.v_docs = [] <span class="comment"># mapping tokens in docs to their vocabulary index</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> m, doc <span class="keyword">in</span> <span class="built_in">enumerate</span>(docs):</span><br><span class="line">            <span class="comment"># sample topic ids for doc; </span></span><br><span class="line">            <span class="comment"># np.random.choice is bootstrap sample method, which can generate array like [2,1,3,3]</span></span><br><span class="line">            _zs = np.random.choice(self.k_ids,<span class="built_in">len</span>(doc))</span><br><span class="line">            self.zs.append(_zs)</span><br><span class="line">            _idx = [self.token2idx[tk] <span class="keyword">for</span> tk <span class="keyword">in</span> doc]</span><br><span class="line">            self.v_docs.append(_idx)</span><br><span class="line">            <span class="keyword">for</span> z,v <span class="keyword">in</span> <span class="built_in">zip</span>(_zs,_idx):</span><br><span class="line">                self.matC[m,z] += <span class="number">1</span></span><br><span class="line">                self.matP[z,v] += <span class="number">1</span></span><br><span class="line">                self.vecP[z] += <span class="number">1</span></span><br><span class="line">            self.vecC[m] += <span class="built_in">len</span>(doc)</span><br></pre></td></tr></table></figure>
<p>然后是LDA的训练部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self,n_iter=<span class="number">100</span></span>):</span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> <span class="built_in">range</span>(n_iter):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Iteration <span class="subst">&#123;it&#125;</span> ...&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> m,vdoc <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.v_docs):</span><br><span class="line">            <span class="keyword">for</span> i,v <span class="keyword">in</span> <span class="built_in">enumerate</span>(vdoc):</span><br><span class="line">                z = self.zs[m][i]</span><br><span class="line">                self.matC[m][z] -= <span class="number">1</span></span><br><span class="line">                self.vecC[m] -= <span class="number">1</span></span><br><span class="line">                self.matP[z][v] -= <span class="number">1</span></span><br><span class="line">                self.vecP[z] -= <span class="number">1</span></span><br><span class="line">                fst_itm = <span class="keyword">lambda</span> k: (self.matP[k][v]+self.beta[v])/(self.matP[k]+self.beta).<span class="built_in">sum</span>()</span><br><span class="line">                scd_itm = <span class="keyword">lambda</span> k: (self.matC[m][k]+self.alpha[k])/(self.matC[m]+self.alpha).<span class="built_in">sum</span>()</span><br><span class="line">                _probs = np.array([fst_itm(k)*scd_itm(k) <span class="keyword">for</span> k <span class="keyword">in</span> self.k_ids])</span><br><span class="line">                probs = _probs / _probs.<span class="built_in">sum</span>()</span><br><span class="line">                zp = np.random.choice(self.k_ids,p=probs)</span><br><span class="line">                self.zs[m][i] = zp</span><br><span class="line">                self.matC[m][zp] += <span class="number">1</span></span><br><span class="line">                self.vecC[m] += <span class="number">1</span></span><br><span class="line">                self.matP[zp][v] += <span class="number">1</span></span><br><span class="line">                self.vecP[zp] += <span class="number">1</span></span><br><span class="line">    _theta = self.matC + self.alpha</span><br><span class="line">    self.theta = _theta / _theta.<span class="built_in">sum</span>(axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">    _phi = self.matP + self.beta</span><br><span class="line">    self.phi = _phi / _phi.<span class="built_in">sum</span>(axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>假如我们已经训练好了模型，后来又增加了新的训练数据，考虑添加一个add_docs接口，使模型在之前的基础上继续训练。在实现时可以有两种选择，一种是沿用之前的词表，只更新文档集，这样实现简单，计算效率也更高；另一种是扩充词表，这么做计算量会更大一些，但也更准确地反映了数据的变化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">add_docs</span>(<span class="params">self,docs,n_iter=<span class="number">100</span></span>):</span><br><span class="line">    old_v = self.V</span><br><span class="line">    <span class="comment"># Update vocabulary</span></span><br><span class="line">    <span class="keyword">for</span> doc <span class="keyword">in</span> docs:</span><br><span class="line">        <span class="keyword">for</span> tk <span class="keyword">in</span> doc:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> (tk <span class="keyword">in</span> self.idx2token):</span><br><span class="line">                self.idx2token.append(tk)</span><br><span class="line">                self.token2idx[tk] = <span class="built_in">len</span>(self.idx2token)-<span class="number">1</span></span><br><span class="line">    self.V = <span class="built_in">len</span>(self.idx2token)</span><br><span class="line">    self.beta = np.ones(self.V) / self.V <span class="comment"># A convience way; other methods might be better?</span></span><br><span class="line">    <span class="comment"># Update idxes of docs: v_docs</span></span><br><span class="line">    nv_docs = [[self.token2idx[tk] <span class="keyword">for</span> tk <span class="keyword">in</span> doc] <span class="keyword">for</span> doc <span class="keyword">in</span> docs]</span><br><span class="line">    self.v_docs += nv_docs</span><br><span class="line">    <span class="comment"># Update shape of matC, matP, topic list: zs</span></span><br><span class="line">    suf_matC = np.zeros((<span class="built_in">len</span>(docs),self.K))</span><br><span class="line">    suf_matP = np.zeros((self.K,self.V-old_v))</span><br><span class="line">    self.matC = np.concatenate([self.matC,suf_matC],axis=<span class="number">0</span>) <span class="comment"># [M+M&#x27;,K]</span></span><br><span class="line">    self.matP = np.concatenate([self.matP,suf_matP],axis=<span class="number">1</span>) <span class="comment"># [K,V+V&#x27;]</span></span><br><span class="line">    vecC = []</span><br><span class="line">    <span class="keyword">for</span> m,doc <span class="keyword">in</span> <span class="built_in">enumerate</span>(nv_docs):</span><br><span class="line">        _zs = np.random.choice(self.k_ids,<span class="built_in">len</span>(doc))</span><br><span class="line">        self.zs.append(_zs)</span><br><span class="line">        <span class="keyword">for</span> v,z <span class="keyword">in</span> <span class="built_in">zip</span>(doc,_zs):</span><br><span class="line">            self.matC[m+self.M][z] += <span class="number">1</span></span><br><span class="line">            self.matP[z][v] += <span class="number">1</span></span><br><span class="line">            self.vecP[z] += <span class="number">1</span></span><br><span class="line">        vecC.append(<span class="built_in">len</span>(doc))</span><br><span class="line">    self.vecC = np.append(self.vecC,vecC)</span><br><span class="line">    self.M = <span class="built_in">len</span>(self.v_docs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># train for another more loops</span></span><br><span class="line">    self.train(n_iter=n_iter)</span><br></pre></td></tr></table></figure>
<p>实现inference接口时，因为推断本质是加入新数据继续迭代至收敛，因此可复用已经实现的add_docs。剩下的接口都是辅助性功能，实现起来比较简单，就一并列出了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">batch_inference</span>(<span class="params">self,docs,n_iter=<span class="number">100</span></span>):</span><br><span class="line">    n_docs = <span class="built_in">len</span>(docs)</span><br><span class="line">    self.add_docs(docs,n_iter=n_iter)</span><br><span class="line">    <span class="keyword">return</span> self.get_corpus_dist()[-n_docs:]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">inference</span>(<span class="params">self,doc,n_iter=<span class="number">100</span></span>):</span><br><span class="line">    bth = [doc]</span><br><span class="line">    dist = self.batch_inference(bth,n_iter=n_iter)</span><br><span class="line">    <span class="keyword">return</span> dist.squeeze()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_corpus_dist</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">return</span> self.theta</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">topic_word_dist</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">return</span> self.phi</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_topic_words</span>(<span class="params">self,n=<span class="number">20</span>,show_weight=<span class="literal">True</span></span>):</span><br><span class="line">    sorted_wghts = np.sort(-<span class="number">1</span> * self.phi,axis=<span class="number">1</span>)[:,:n]*(-<span class="number">1</span>) </span><br><span class="line">    top_idx = (-<span class="number">1</span> * self.phi).argsort(axis=<span class="number">1</span>)[:,:n]</span><br><span class="line">    suf = <span class="keyword">lambda</span> wght: <span class="string">f&#x27;*<span class="subst">&#123;wght:<span class="number">.07</span>f&#125;</span>&#x27;</span> <span class="keyword">if</span> show_weight <span class="keyword">else</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line">    topic_words = [[<span class="string">f&quot;<span class="subst">&#123;self.idx2token[idx]&#125;</span><span class="subst">&#123;suf(wght)&#125;</span>&quot;</span> <span class="keyword">for</span> idx,wght <span class="keyword">in</span> <span class="built_in">zip</span>(idxes,wghts)] <span class="keyword">for</span> idxes,wghts <span class="keyword">in</span> <span class="built_in">zip</span>(top_idx,sorted_wghts)]</span><br><span class="line">    <span class="keyword">for</span> tp_wd <span class="keyword">in</span> topic_words:</span><br><span class="line">        <span class="built_in">print</span>(tp_wd)</span><br><span class="line">    <span class="keyword">return</span> topic_words</span><br></pre></td></tr></table></figure>
<p>让我们将模型应用到TNEWS上，迭代次数设定为100次，查看模型对文档主题分布$\theta$的推断，以及权重top20的主题词。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    N_Iter = <span class="number">100</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Train on trn data ...&#x27;</span>)</span><br><span class="line">    lda_model = LDA(docs=trn_tokenized_docs,K=<span class="number">20</span>)</span><br><span class="line">    lda_model.train(n_iter=N_Iter)</span><br><span class="line">    tp_wd_dist = lda_model.topic_word_dist() <span class="comment"># return [K,V] matrix</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;tp_wd_dist:&#x27;</span>,tp_wd_dist)</span><br><span class="line">    lda_model.show_topic_words(n=<span class="number">20</span>)</span><br><span class="line">    doc_tp_dist = lda_model.get_corpus_dist() <span class="comment"># return [M,K] matrix, where M = # of all the docs have feed in.</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;doc_tp_dist:&#x27;</span>,doc_tp_dist)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;=&#x27;</span>*<span class="number">40</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Add Dev data&#x27;</span>)</span><br><span class="line">    lda_model.add_docs(dev_tokenized_docs,n_iter=N_Iter)</span><br><span class="line">    tp_wd_dist = lda_model.topic_word_dist() <span class="comment"># return [K,V] matrix</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;tp_wd_dist:&#x27;</span>,tp_wd_dist)</span><br><span class="line">    lda_model.show_topic_words(n=<span class="number">20</span>)</span><br><span class="line">    doc_tp_dist = lda_model.get_corpus_dist() <span class="comment"># return [M,K] matrix, where M = # of all the docs have feed in.</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;doc_tp_dist:&#x27;</span>,doc_tp_dist)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;=&#x27;</span>*<span class="number">40</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Inference on Test data&#x27;</span>)</span><br><span class="line">    doc_tp_dist = lda_model.batch_inference(tst_tokenized_docs,n_iter=N_Iter) <span class="comment"># return [M&#x27;,K] matrix, where M&#x27; = # of new_tokenized_docs</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;doc_tp_dist:&#x27;</span>,doc_tp_dist)</span><br><span class="line">    new_tokenized_doc = tst_tokenized_docs[<span class="number">17</span>]</span><br><span class="line">    tp_dist = lda_model.inference(new_tokenized_doc,n_iter=N_Iter) <span class="comment"># return [K] vector</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;tp_dist:&#x27;</span>,tp_dist)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&gt;python LDA.py</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Train on trn data ...</span></span><br><span class="line"><span class="string">Vocabulary length: 4771</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tp_wd_dist: [[6.29428422e-07 6.29428422e-07 6.29428422e-07 ... 6.29428422e-07</span></span><br><span class="line"><span class="string">  6.29428422e-07 6.29428422e-07]</span></span><br><span class="line"><span class="string"> [5.68020771e-07 5.68020771e-07 5.68020771e-07 ... 2.71059512e-03</span></span><br><span class="line"><span class="string">  5.68020771e-07 5.68020771e-07]</span></span><br><span class="line"><span class="string"> [5.64958665e-07 5.64958665e-07 4.58226674e-02 ... 5.64958665e-07</span></span><br><span class="line"><span class="string">  5.64958665e-07 5.64958665e-07]</span></span><br><span class="line"><span class="string"> ...</span></span><br><span class="line"><span class="string"> [4.93175682e-07 4.93175682e-07 4.93175682e-07 ... 4.93175682e-07</span></span><br><span class="line"><span class="string">  4.93175682e-07 4.93175682e-07]</span></span><br><span class="line"><span class="string"> [5.22692431e-07 5.22692431e-07 5.22692431e-07 ... 5.22692431e-07</span></span><br><span class="line"><span class="string">  5.22692431e-07 5.22692431e-07]</span></span><br><span class="line"><span class="string"> [4.69954405e-07 4.69954405e-07 4.69954405e-07 ... 4.69954405e-07</span></span><br><span class="line"><span class="string">  4.69954405e-07 4.69954405e-07]]</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">[&#x27;年*0.0360367&#x27;, &#x27;美国*0.0240247&#x27;, &#x27;创新*0.0150156&#x27;, &#x27;房产*0.0150156&#x27;, &#x27;影响*0.0150156&#x27;, &#x27;能力*0.0120126&#x27;, &#x27;文化*0.0120126&#x27;, &#x27;一个*0.0120126&#x27;, &#x27;评价*0.0120126&#x27;, &#x27;飞*0.0090096&#x27;, &#x27;国*0.0090096&#x27;, &#x27;娶*0.0090096&#x27;, &#x27;Q*0.0090096&#x27;, &#x27;辆*0.0090096&#x27;, &#x27;计划*0.0090096&#x27;, &#x27;世界杯*0.0090096&#x27;, &#x27;基地*0.0090096&#x27;, &#x27;装备*0.0090096&#x27;, &#x27;到来*0.0090096&#x27;, &#x27;人性*0.0090096&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;四*0.0298109&#x27;, &#x27;媒体*0.0162607&#x27;, &#x27;建设*0.0162607&#x27;, &#x27;楼市*0.0135507&#x27;, &#x27;时间*0.0135507&#x27;, &#x27;小镇*0.0135507&#x27;, &#x27;西方*0.0108407&#x27;, &#x27;成功*0.0108407&#x27;, &#x27;时刻*0.0108407&#x27;, &#x27;比特币*0.0108407&#x27;, &#x27;爱*0.0108407&#x27;, &#x27;首届*0.0108407&#x27;, &#x27;赚钱*0.0108407&#x27;, &#x27;农民*0.0108407&#x27;, &#x27;没*0.0108407&#x27;, &#x27;大战*0.0081306&#x27;, &#x27;机遇*0.0081306&#x27;, &#x27;沪</span></span><br><span class="line"><span class="string">*0.0081306&#x27;, &#x27;信*0.0081306&#x27;, &#x27;专家*0.0081306&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;中*0.0512135&#x27;, &#x27;手机*0.0458227&#x27;, &#x27;太*0.0269547&#x27;, &#x27;詹姆斯*0.0188685&#x27;, &#x27;送*0.0161731&#x27;, &#x27;科技*0.0134777&#x27;, &#x27;里*0.0134777&#x27;, &#x27;问题*0.0134777&#x27;, &#x27;已经*0.0134777&#x27;, &#x27;不能*0.0134777&#x27;, &#x27;家长*0.0107822&#x27;, &#x27;万*0.0107822&#x27;, &#x27;万元*0.0107822&#x27;, &#x27;路*0.0107822&#x27;, &#x27;学会*0.0107822&#x27;, &#x27;项*0.0107822&#x27;, &#x27;农业*0.0080868&#x27;, &#x27;制造*0.0080868&#x27;, &#x27;幸福*0.0080868&#x27;, &#x27;思考*0.0080868&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;一个*0.0488894&#x27;, &#x27;中国*0.0444449&#x27;, &#x27;三*0.0288894&#x27;, &#x27;旅游*0.0266671&#x27;, &#x27;新*0.0244449&#x27;, &#x27;更*0.0200005&#x27;, &#x27;没有*0.0200005&#x27;, &#x27;即将*0.0177782&#x27;, &#x27;经济*0.0133338&#x27;, &#x27;重要*0.0111116&#x27;, &#x27;产品*0.0111116&#x27;, &#x27;喜欢*0.0111116&#x27;, &#x27;老*0.0111116&#x27;, &#x27;实力*0.0111116&#x27;, &#x27;司机*0.0111116&#x27;, &#x27;曝光*0.0111116&#x27;, &#x27;请*0.0111116&#x27;, &#x27;产业*0.0088894&#x27;, &#x27;品牌*0.0088894&#x27;, &#x27;难道*0.0088894&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;两*0.0474045&#x27;, &#x27;做*0.0316032&#x27;, &#x27;款*0.0248312&#x27;, &#x27;王者*0.0203165&#x27;, &#x27;俄*0.0180592&#x27;, &#x27;开*0.0158018&#x27;, &#x27;地球*0.0135445&#x27;, &#x27;推荐*0.0135445&#x27;, &#x27;孩子*0.0135445&#x27;, </span></span><br><span class="line"><span class="string">&#x27;成*0.0112872&#x27;, &#x27;投资*0.0112872&#x27;, &#x27;值得*0.0112872&#x27;, &#x27;要求*0.0112872&#x27;, &#x27;谢娜*0.0090298&#x27;, &#x27;区别*0.0090298&#x27;, &#x27;首发*0.0090298&#x27;, &#x27;重*0.0090298&#x27;, &#x27;型*0.0090298&#x27;, &#x27;见过*0.0090298&#x27;, &#x27;先*0.0090298&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;现在*0.0473543&#x27;, &#x27;历史*0.0222847&#x27;, &#x27;房价*0.0167137&#x27;, &#x27;儿子*0.0167137&#x27;, &#x27;进*0.0167137&#x27;, &#x27;举行*0.0167137&#x27;, &#x27;元*0.0139282&#x27;, &#x27;发*0.0111426&#x27;, &#x27;梦*0.0111426&#x27;, &#x27;航母*0.0111426&#x27;, &#x27;联想*0.0083571&#x27;, &#x27;逆袭*0.0083571&#x27;, &#x27;周*0.0083571&#x27;, &#x27;旗*0.0083571&#x27;, &#x27;行情*0.0083571&#x27;, &#x27;红毯*0.0083571&#x27;, &#x27;再次*0.0083571&#x27;, &#x27;收入*0.0083571&#x27;, &#x27;现场*0.0083571&#x27;, &#x27;风险*0.0083571&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;五*0.0294845&#x27;, &#x27;到底*0.0196565&#x27;, &#x27;网友*0.0171995&#x27;, &#x27;核*0.0171995&#x27;, &#x27;协议*0.0147425&#x27;, &#x27;小米*0.0147425&#x27;, &#x27;联*0.0147425&#x27;, &#x27;地方*0.0147425&#x27;, &#x27;微信*0.0122855&#x27;, &#x27;亿*0.0122855&#x27;, &#x27;伊*0.0122855&#x27;, &#x27;｜*0.0122855&#x27;, &#x27;融资*0.0122855&#x27;, &#x27;出现*0.0098285&#x27;, &#x27;爆*0.0098285&#x27;, &#x27;好玩*0.0098285&#x27;, &#x27;阿里巴巴*0.0098285&#x27;, &#x27;改变*0.0098285&#x27;, &#x27;教*0.0098285&#x27;, &#x27;申请*0.0098285&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;」*0.0329119&#x27;, &#x27;第一*0.0303803&#x27;, &#x27;特朗普*0.0303803&#x27;, &#x27;岁*0.0253170&#x27;, &#x27;成功*0.0151904&#x27;, &#x27;复*0.0151904&#x27;, &#x27;上市*0.0126588&#x27;, &#x27;油*0.0126588&#x27;, &#x27;跑*0.0126588&#x27;, &#x27;称*0.0126588&#x27;, &#x27;印度*0.0101271&#x27;, &#x27;智能*0.0101271&#x27;, &#x27;电*0.0101271&#x27;, &#x27;接*0.0101271&#x27;, &#x27;参加*0.0101271&#x27;, &#x27;进口*0.0101271&#x27;, &#x27;电子*0.0101271&#x27;, &#x27;上映*0.0075955&#x27;, &#x27;获得*0.0075955&#x27;, &#x27;号*0.0075955&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;会*0.0410262&#x27;, &#x27;说*0.0333339&#x27;, &#x27;使用*0.0179493&#x27;, &#x27;位*0.0179493&#x27;, &#x27;买房*0.0128211&#x27;, &#x27;月*0.0128211&#x27;, &#x27;终于*0.0128211&#x27;, &#x27;便宜*0.0128211&#x27;, &#x27;价格*0.0128211&#x27;, &#x27;卡*0.0128211&#x27;, &#x27;低*0.0102569&#x27;, &#x27;酒*0.0102569&#x27;, &#x27;主播*0.0102569&#x27;, &#x27;苹果*0.0102569&#x27;, &#x27;腿*0.0076928&#x27;, &#x27;游*0.0076928&#x27;, &#x27;高校*0.0076928&#x27;, &#x27;竟然*0.0076928&#x27;, &#x27;晋级*0.0076928&#x27;, &#x27;分*0.0076928&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;亿*0.0453339&#x27;, &#x27;美*0.0373339&#x27;, &#x27;点*0.0320006&#x27;, &#x27;没*0.0186672&#x27;, &#x27;事件*0.0160006&#x27;, &#x27;事*0.0160006&#x27;, &#x27;版*0.0133339&#x27;, &#x27;发明*0.0133339&#x27;, &#x27;走*0.0133339&#x27;, &#x27;猪*0.0133339&#x27;, &#x27;恒大*0.0106672&#x27;, &#x27;手游*0.0106672&#x27;, &#x27;怒*0.0106672&#x27;, &#x27;角色*0.0106672&#x27;, &#x27;取消*0.0106672&#x27;, &#x27;无数*0.0106672&#x27;, &#x27;导弹*0.0106672&#x27;, &#x27;响*0.0106672&#x27;, &#x27;穿*0.0080006&#x27;, &#x27;海南*0.0080006&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;中国*0.0316032&#x27;, &#x27;看待*0.0316032&#x27;, &#x27;俄罗斯*0.0293458&#x27;, &#x27;学生*0.0225738&#x27;, &#x27;发展*0.0203165&#x27;, &#x27;级*0.0203165&#x27;, &#x27;里*0.0203165&#x27;, &#x27;老师*0.0180592&#x27;, &#x27;知道*0.0158018&#x27;, &#x27;普京*0.0158018&#x27;, &#x27;种*0.0158018&#x27;, &#x27;是否*0.0158018&#x27;, &#x27;生活*0.0158018&#x27;, &#x27;轮*0.0135445&#x27;, &#x27;公司*0.0135445&#x27;, &#x27;古代*0.0135445&#x27;, &#x27;这种*0.0135445&#x27;, &#x27;片*0.0112872&#x27;, &#x27;最好*0.0112872&#x27;, &#x27;故事*0.0112872&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;荣耀*0.0243249&#x27;, &#x27;比较*0.0243249&#x27;, &#x27;教育*0.0216222&#x27;, &#x27;首*0.0189195&#x27;, &#x27;应该*0.0162168&#x27;, &#x27;全球*0.0162168&#x27;, &#x27;国产*0.0162168&#x27;, &#x27;G*0.0135141&#x27;, &#x27;数据*0.0135141&#x27;, &#x27;行业*0.0135141&#x27;, &#x27;半*0.0108114&#x27;, &#x27;一战*0.0108114&#x27;, &#x27;反*0.0108114&#x27;, &#x27;南昌*0.0108114&#x27;, &#x27;偶遇*0.0081087&#x27;, &#x27;公布*0.0081087&#x27;, &#x27;大爷*0.0081087&#x27;, &#x27;听说*0.0081087&#x27;, &#x27;发声*0.0081087&#x27;, &#x27;创*0.0081087&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;年*0.0725005&#x27;, &#x27;十*0.0475005&#x27;, &#x27;日本*0.0400005&#x27;, &#x27;汽车*0.0150005&#x27;, &#x27;项目*0.0150005&#x27;, &#x27;银行*0.0125005&#x27;, &#x27;选择*0.0125005&#x27;, &#x27;实现*0.0125005&#x27;, &#x27;晒*0.0125005&#x27;, &#x27;地震*0.0125005&#x27;, &#x27;女友*0.0100005&#x27;, &#x27;国内*0.0100005&#x27;, &#x27;山*0.0100005&#x27;, &#x27;德国*0.0100005&#x27;, &#x27;结婚*0.0100005&#x27;, &#x27;微博*0.0100005&#x27;, &#x27;张*0.0100005&#x27;, &#x27;富士康*0.0075005&#x27;, &#x27;史上*0.0075005&#x27;, &#x27;互联网*0.0075005&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;会*0.0232564&#x27;, &#x27;A*0.0206724&#x27;, &#x27;比赛*0.0155044&#x27;, &#x27;原来*0.0129204&#x27;, &#x27;腾讯*0.0129204&#x27;, &#x27;发生*0.0129204&#x27;, &#x27;济南*0.0129204&#x27;, &#x27;NBA*0.0129204&#x27;, &#x27;活动*0.0129204&#x27;, &#x27;方面*0.0103365&#x27;, &#x27;直接*0.0103365&#x27;, &#x27;水平*0.0103365&#x27;, &#x27;没有*0.0103365&#x27;, &#x27;·*0.0103365&#x27;, &#x27;仅*0.0103365&#x27;, &#x27;功能*0.0103365&#x27;, &#x27;尴尬*0.0077525&#x27;, &#x27;陕西*0.0077525&#x27;, &#x27;令狐冲*0.0077525&#x27;, &#x27;棋牌*0.0077525&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;上联*0.0362817&#x27;, &#x27;下联*0.0362817&#x27;, &#x27;城市*0.0294789&#x27;, &#x27;三*0.0181411&#x27;, &#x27;游客*0.0158735&#x27;, &#x27;公里*0.0158735&#x27;, &#x27;深圳*0.0158735&#x27;, &#x27;勇士*0.0136059&#x27;, &#x27;千*0.0136059&#x27;, &#x27;火箭*0.0136059&#x27;, &#x27;座*0.0136059&#x27;, &#x27;上海*0.0113383&#x27;, &#x27;泰山*0.0113383&#x27;, &#x27;黄山*0.0113383&#x27;, &#x27;造*0.0113383&#x27;, &#x27;赵本山*0.0113383&#x27;, &#x27;分钟*0.0113383&#x27;, &#x27;正确</span></span><br><span class="line"><span class="string">*0.0090708&#x27;, &#x27;内容*0.0090708&#x27;, &#x27;有人*0.0090708&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;美国*0.0388606&#x27;, &#x27;伊朗*0.0310886&#x27;, &#x27;车*0.0284980&#x27;, &#x27;真的*0.0207259&#x27;, &#x27;超*0.0155446&#x27;, &#x27;名*0.0129539&#x27;, &#x27;叙利亚*0.0129539&#x27;, &#x27;小时*0.0129539&#x27;, &#x27;最多*0.0129539&#x27;, &#x27;空袭*0.0129539&#x27;, &#x27;面临*0.0129539&#x27;, &#x27;适合*0.0129539&#x27;, &#x27;外*0.0103632&#x27;, &#x27;升级*0.0103632&#x27;, &#x27;进入*0.0103632&#x27;, &#x27;鸡*0.0077726&#x27;, &#x27;退出*0.0077726&#x27;, &#x27;季*0.0077726&#x27;, &#x27;大型*0.0077726&#x27;, &#x27;计算机*0.0077726&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;买*0.0467039&#x27;, &#x27;万*0.0439566&#x27;, &#x27;国家*0.0247259&#x27;, &#x27;中*0.0219786&#x27;, &#x27;米*0.0192313&#x27;, &#x27;吃*0.0164841&#x27;, &#x27;出*0.0164841&#x27;, &#x27;行*0.0137368&#x27;, &#x27;知道*0.0137368&#x27;, &#x27;最 </span></span><br><span class="line"><span class="string">后*0.0137368&#x27;, &#x27;启动*0.0137368&#x27;, &#x27;现身*0.0109896&#x27;, &#x27;河南*0.0109896&#x27;, &#x27;战争*0.0109896&#x27;, &#x27;作品*0.0109896&#x27;, &#x27;时*0.0109896&#x27;, &#x27;抵达*0.0109896&#x27;, &#x27;文明*0.0109896&#x27;, &#x27;工作*0.0082423&#x27;, &#x27;每月*0.0082423&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;中国*0.0635299&#x27;, &#x27;世界*0.0447064&#x27;, &#x27;高*0.0211770&#x27;, &#x27;链*0.0188240&#x27;, &#x27;成为*0.0188240&#x27;, &#x27;原因*0.0188240&#x27;, &#x27;前*0.0164711&#x27;, &#x27;区块*0.0164711&#x27;, &#x27;美元*0.0164711&#x27;, &#x27;技术*0.0141181&#x27;, &#x27;曾经*0.0141181&#x27;, &#x27;今年*0.0117652&#x27;, &#x27;平台*0.0117652&#x27;, &#x27;认为*0.0094123&#x27;, &#x27;太空*0.0094123&#x27;, &#x27;唯一*0.0094123&#x27;, &#x27;无法*0.0094123&#x27;, &#x27;北京</span></span><br><span class="line"><span class="string">*0.0094123&#x27;, &#x27;解说*0.0094123&#x27;, &#x27;排名*0.0094123&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;日*0.0374070&#x27;, &#x27;游戏*0.0374070&#x27;, &#x27;「*0.0349132&#x27;, &#x27;钱*0.0224444&#x27;, &#x27;以色列*0.0174569&#x27;, &#x27;农村*0.0174569&#x27;, &#x27;省*0.0149631&#x27;, &#x27;建*0.0124694&#x27;, &#x27;天*0.0124694&#x27;, </span></span><br><span class="line"><span class="string">&#x27;东西*0.0124694&#x27;, &#x27;市场*0.0124694&#x27;, &#x27;考*0.0099756&#x27;, &#x27;泰国*0.0099756&#x27;, &#x27;创业*0.0099756&#x27;, &#x27;快*0.0099756&#x27;, &#x27;车*0.0099756&#x27;, &#x27;处理*0.0074818&#x27;, &#x27;机器人*0.0074818&#x27;, &#x27;届*0.0074818&#x27;, &#x27;灯*0.0074818&#x27;]</span></span><br><span class="line"><span class="string">[&#x27;次*0.0336328&#x27;, &#x27;会*0.0291485&#x27;, &#x27;联盟*0.0246641&#x27;, &#x27;英雄*0.0246641&#x27;, &#x27;玩*0.0201798&#x27;, &#x27;需要*0.0201798&#x27;, &#x27;股*0.0134534&#x27;, &#x27;复仇者*0.0134534&#x27;, &#x27;冠军*0.0134534&#x27;, &#x27;战场*0.0112112&#x27;, &#x27;回应*0.0112112&#x27;, &#x27;很多*0.0089691&#x27;, &#x27;获*0.0089691&#x27;, &#x27;赛*0.0089691&#x27;, &#x27;带*0.0089691&#x27;, &#x27;看到*0.0089691&#x27;, &#x27;贷款*0.0089691&#x27;, &#x27;虎牙*0.0089691&#x27;, &#x27;怕*0.0089691&#x27;, &#x27;央视*0.0089691&#x27;]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">doc_tp_dist: [[0.00294118 0.00294118 0.47352941 ... 0.00294118 0.00294118 0.00294118]</span></span><br><span class="line"><span class="string"> [0.065625   0.003125   0.003125   ... 0.065625   0.003125   0.003125  ]</span></span><br><span class="line"><span class="string"> [0.00454545 0.00454545 0.00454545 ... 0.00454545 0.36818182 0.00454545]</span></span><br><span class="line"><span class="string"> ...</span></span><br><span class="line"><span class="string"> [0.00714286 0.72142857 0.00714286 ... 0.00714286 0.00714286 0.00714286]</span></span><br><span class="line"><span class="string"> [0.00833333 0.00833333 0.00833333 ... 0.00833333 0.00833333 0.00833333]</span></span><br><span class="line"><span class="string"> [0.00625    0.00625    0.00625    ... 0.13125    0.00625    0.00625   ]]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>直观来说，从结果上看，部分主题的主题词具有一定关联性，而其余主题则较为隐晦；而定量地评测主题建模结果的优劣是另一个大话题，本文不予展开。如果我们更换一下停用词，比如把停用词换成baidu_stopwords，结果也会发生较大改变，这表明主题模型的效果受停用词库的影响较大，在选择停用词时应充分考虑数据所属的领域。</p>
<h4 id="参考文献">参考文献</h4>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/projects/">Projects</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#LDA%E7%9A%84%E5%8A%A8%E6%9C%BA%E5%92%8C%E6%80%9D%E8%B7%AF"><span class="toc-number">1.</span> <span class="toc-text">LDA的动机和思路</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">2.</span> <span class="toc-text">参考文献</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://zll17.github.io/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://zll17.github.io/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/&text=LDA实作指南"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://zll17.github.io/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/&title=LDA实作指南"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://zll17.github.io/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/&is_video=false&description=LDA实作指南"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=LDA实作指南&body=Check out this article: https://zll17.github.io/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://zll17.github.io/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/&title=LDA实作指南"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://zll17.github.io/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/&title=LDA实作指南"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://zll17.github.io/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/&title=LDA实作指南"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://zll17.github.io/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/&title=LDA实作指南"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://zll17.github.io/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/&name=LDA实作指南&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://zll17.github.io/2022/05/22/LDA%E5%AE%9E%E4%BD%9C%E6%8C%87%E5%8D%97/&t=LDA实作指南"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2016-2022
    zhangleilan
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/projects/">Projects</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->
 
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script> 




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script> 
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Umami Analytics -->

<!-- Disqus Comments -->


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
